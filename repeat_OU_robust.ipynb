{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import CompDoobTransform as cdt\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from CompDoobTransform.utils import normal_logpdf, resampling\n",
    "plt.style.use('ggplot')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Computing on ' + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for objects relating to latent state process\n",
    "state = {}\n",
    "\n",
    "# dimension of state \n",
    "d = 1 \n",
    "state['dim'] = d\n",
    "\n",
    "# drift of diffusion\n",
    "beta = torch.tensor(1.0, device = device) \n",
    "b = lambda x: - beta * x # drift\n",
    "state['drift'] = b\n",
    "\n",
    "# diffusion coefficient of diffusion\n",
    "sigma = torch.tensor(1.0, device = device) # diffusion coefficient\n",
    "state['sigma'] = sigma\n",
    "\n",
    "# simulate initial states (from stationary distribution)\n",
    "initial = lambda N: (sigma / torch.sqrt(2.0 * beta)) * torch.randn(N, d, device = device)\n",
    "state['initial'] = initial\n",
    "\n",
    "# time interval\n",
    "T = torch.tensor(1.0, device = device) \n",
    "state['terminal_time'] = T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for objects relating to observations\n",
    "obs = {}\n",
    "\n",
    "# dimension of observation\n",
    "p = 1\n",
    "obs['dim'] = p\n",
    "\n",
    "# observation parameters\n",
    "std_obs = 0.25\n",
    "var_obs = torch.tensor(std_obs**2, device = device) # variance of observation\n",
    "\n",
    "# log-observation density\n",
    "obs_log_density = lambda x, y: normal_logpdf(y, x, var_obs) # terminal condition, returns size (N)\n",
    "obs['log_density'] = obs_log_density\n",
    "\n",
    "# simulate observations\n",
    "observation = lambda N: initial(N) + torch.sqrt(var_obs) * torch.randn(N, p, device = device)\n",
    "obs['observation'] = observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define standardization means and standard deviations\n",
    "standardization = {'x_mean': torch.zeros(1), \n",
    "                   'x_std': (sigma / torch.sqrt(2.0 * beta)) * torch.ones(1), \n",
    "                   'y_mean': torch.zeros(1), \n",
    "                   'y_std': torch.sqrt(sigma**2 / (2.0 * beta) + var_obs) * torch.ones(1)}\n",
    "\n",
    "standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition mean and variance \n",
    "mean_x = lambda t,x: x * torch.exp(- beta * t)\n",
    "var_x = lambda t: (1.0 - torch.exp(- 2.0 * beta * t)) / (2 * beta)\n",
    "\n",
    "# posterior mean and variance at time T\n",
    "var_h = lambda t: 1.0 / (1.0 / var_x(t) + 1.0 / var_obs)\n",
    "mean_h = lambda x0,y: var_h(T) * (mean_x(T, x0) / var_x(T) + y / var_obs)\n",
    "\n",
    "# h function\n",
    "log_h = lambda t,x,y: - 0.5 * d * torch.log(torch.tensor(2 * math.pi, device = device)) \\\n",
    "                      - 0.5 * d * torch.log(var_x(T-t)) - 0.5 * d * torch.log(var_obs) + 0.5 * d * torch.log(var_h(T-t)) \\\n",
    "                      + 0.5 * var_h(T-t) * torch.sum(torch.square(mean_x(T-t,x) / var_x(T-t) + y / var_obs), dim = 1, keepdim = True) \\\n",
    "                      - 0.5 * torch.sum(torch.square(mean_x(T-t,x)), dim = 1, keepdim = True) / var_x(T-t) \\\n",
    "                      - 0.5 * torch.sum(torch.square(y)) / var_obs\n",
    "\n",
    "# true V0 network\n",
    "V0_net_true = lambda x,y: -log_h(0.0, x, y.reshape(1,p)).squeeze()\n",
    "\n",
    "# true Z network \n",
    "Z_net_true = lambda t,x,y: - var_h(T-t) * (mean_x(T-t,x) / var_x(T-t) + y / var_obs) * torch.exp(- beta * (T-t)) / var_x(T-t) \\\n",
    "                           + mean_x(T-t,x) * torch.exp(- beta * (T-t)) / var_x(T-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-discretization settings\n",
    "M = 50 # number of time steps\n",
    "\n",
    "# V0 and Z neural network configuration\n",
    "V0_net_config = {'layers': [16], 'standardization': standardization}\n",
    "Z_net_config = {'layers': [d+16], 'standardization': standardization}\n",
    "net_config = {'V0': V0_net_config, 'Z': Z_net_config}\n",
    "\n",
    "# learning type\n",
    "# learning_type = 'standard'\n",
    "learning_type = 'iterative'                \n",
    "\n",
    "# create model instance\n",
    "model = cdt.core.model(state, obs, M, net_config, device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization configuration (standard training)\n",
    "I = 2000\n",
    "optim_config = {'minibatch': 100, \n",
    "                'num_obs_per_batch': 10, \n",
    "                'num_iterations': I,\n",
    "                'learning_rate' : 0.01, \n",
    "                'initial_required' : True}\n",
    "# training\n",
    "time_start = time.time() \n",
    "if learning_type == 'standard':\n",
    "    model.train_standard(optim_config)\n",
    "if learning_type == 'iterative':\n",
    "    model.train_iterative(optim_config)\n",
    "time_end = time.time()\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Training time (secs): \" + str(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss over optimization iterations\n",
    "plt.figure()\n",
    "plt.plot(torch.arange(I), model.loss.to('cpu'), 'k.')\n",
    "plt.xlabel('iteration', fontsize = 15)\n",
    "plt.ylabel('loss', fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary particle filter with true networks\n",
    "def run_true_APF(initial_states, observations, num_samples):\n",
    "    \"\"\"\n",
    "    Run auxiliary particle filter with true networks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    initial_states : initial states of X process (N, d)\n",
    "    \n",
    "    observations : sequence of observations to be filtered (K, p)\n",
    "\n",
    "    num_samples : sample size (int)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict containing:    \n",
    "        states : X process at observation times (N, K+1, d) or (N, K*M+1, d) if full_path == True\n",
    "        ess : effective sample sizes at unit times (K+1)        \n",
    "        log_norm_const : log-normalizing constant estimates (K+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize and preallocate\n",
    "    N = num_samples\n",
    "    Y = observations\n",
    "    K = observations.shape[0]        \n",
    "    X = initial_states        \n",
    "    states = torch.zeros(N, K+1, d, device = device)\n",
    "    states[:, 0, :] = X\n",
    "    ess = torch.zeros(K+1, device = device)\n",
    "    ess[0] = N\n",
    "    log_norm_const = torch.zeros(K+1, device = device)\n",
    "    log_ratio_norm_const = torch.tensor(0.0, device = device)\n",
    "    \n",
    "    # each observation\n",
    "    for k in range(K):        \n",
    "\n",
    "        # evaluate initial values V0\n",
    "        V0 = V0_net_true(X, Y[k,:]) # size (N)            \n",
    "        V = V0.clone()\n",
    "        \n",
    "        # each time interval\n",
    "        for m in range(M):\n",
    "            # time step \n",
    "            stepsize = model.stepsizes[m]\n",
    "            t = model.time[m]\n",
    "\n",
    "            # Brownian increment\n",
    "            W = torch.sqrt(stepsize) * torch.randn(N, d, device = device) # size (N, d)\n",
    "            \n",
    "            # simulate V process forwards in time            \n",
    "            Z = Z_net_true(t, X, Y[k,:]) # size (N, d)\n",
    "            control = - Z.clone()\n",
    "            drift_V = - 0.5 * torch.sum(torch.square(Z), 1) # size (N)                \n",
    "            euler_V = V + stepsize * drift_V # size (N)\n",
    "            V = euler_V + torch.sum(Z * W, 1) # size (N)\n",
    "\n",
    "            # simulate X process forwards in time\n",
    "            drift_X = b(X) + sigma * control\n",
    "            euler_X = X + stepsize * drift_X\n",
    "            X = euler_X + sigma * W\n",
    "\n",
    "        # compute and normalize weights, compute ESS and normalizing constant\n",
    "        log_weights = V + obs_log_density(X, Y[k,:]) - V0\n",
    "        max_log_weights = torch.max(log_weights)\n",
    "        weights = torch.exp(log_weights - max_log_weights)\n",
    "        normalized_weights = weights / torch.sum(weights)\n",
    "        ess[k+1] = 1.0 / torch.sum(normalized_weights**2)\n",
    "        log_ratio_norm_const = log_ratio_norm_const + torch.log(torch.mean(weights)) + max_log_weights\n",
    "        log_norm_const[k+1] = log_ratio_norm_const\n",
    "\n",
    "        # resampling            \n",
    "        ancestors = resampling(normalized_weights, N)\n",
    "        X = X[ancestors,:]\n",
    "\n",
    "        # store states \n",
    "        states[:, k+1, :] = X\n",
    "\n",
    "    # output\n",
    "    output = {'states' : states, 'ess' : ess, 'log_norm_const' : log_norm_const}\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully adapted auxiliary particle filter without time-discretization\n",
    "def run_FA_APF(initial_states, observations, num_samples):\n",
    "    \"\"\"\n",
    "    Run fully adapted auxiliary particle filter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    initial_states : initial states of X process (N, d)\n",
    "    \n",
    "    observations : sequence of observations to be filtered (K, p)\n",
    "\n",
    "    num_samples : sample size (int)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict containing:    \n",
    "        states : X process at observation times (N, K+1, d) or (N, K*M+1, d) if full_path == True\n",
    "        ess : effective sample sizes at unit times (K+1)        \n",
    "        log_norm_const : log-normalizing constant estimates (K+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize and preallocate\n",
    "    N = num_samples\n",
    "    Y = observations\n",
    "    K = observations.shape[0]        \n",
    "    X = initial_states        \n",
    "    states = torch.zeros(N, K+1, d, device = device)\n",
    "    states[:, 0, :] = X\n",
    "    ess = torch.zeros(K+1, device = device)\n",
    "    ess[0] = N\n",
    "    log_norm_const = torch.zeros(K+1, device = device)\n",
    "    log_ratio_norm_const = torch.tensor(0.0, device = device)\n",
    "    \n",
    "    # each observation\n",
    "    for k in range(K): \n",
    "        # compute and normalize weights, compute ESS and normalizing constant\n",
    "        log_weights = log_h(0.0, X, Y[k,:].reshape(1,p)).squeeze()\n",
    "        max_log_weights = torch.max(log_weights)\n",
    "        weights = torch.exp(log_weights - max_log_weights)\n",
    "        normalized_weights = weights / torch.sum(weights)\n",
    "        ess[k+1] = 1.0 / torch.sum(normalized_weights**2)\n",
    "        log_ratio_norm_const = log_ratio_norm_const + torch.log(torch.mean(weights)) + max_log_weights\n",
    "        log_norm_const[k+1] = log_ratio_norm_const\n",
    "\n",
    "        # resampling            \n",
    "        ancestors = resampling(normalized_weights, N)\n",
    "        X = X[ancestors,:]\n",
    "\n",
    "        # move using locally optimal proposal transition\n",
    "        X = mean_h(X, Y[k,:].reshape(1,p)) + torch.sqrt(var_h(T)) * torch.randn(N, d, device = device)\n",
    "\n",
    "        # store states \n",
    "        states[:, k+1, :] = X\n",
    "\n",
    "    # output\n",
    "    output = {'states' : states, 'ess' : ess, 'log_norm_const' : log_norm_const}\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat particle filters\n",
    "multiplier_level = list(range(1,11)) # controls level of misspecification\n",
    "num_multiplier = len(multiplier_level)\n",
    "K = 100 # number of observations\n",
    "R = 100 # number of repeats\n",
    "N = 2**6 # number of particles\n",
    "BPF = {'ess' : torch.zeros(num_multiplier, R), 'log_estimate' : torch.zeros(num_multiplier, R)}\n",
    "APF = {'ess' : torch.zeros(num_multiplier, R), 'log_estimate' : torch.zeros(num_multiplier, R)}\n",
    "true_APF = {'ess' : torch.zeros(num_multiplier, R), 'log_estimate' : torch.zeros(num_multiplier, R)}\n",
    "FA_APF = {'ess' : torch.zeros(num_multiplier, R), 'log_estimate' : torch.zeros(num_multiplier, R)}\n",
    "\n",
    "for i in range(num_multiplier):\n",
    "    # level of misspecification\n",
    "    multiplier = float(multiplier_level[i])\n",
    "\n",
    "    # simulate latent process and observations\n",
    "    X0 = initial(1)\n",
    "    X = torch.zeros(K+1, d)\n",
    "    X[0,:] = X0.clone()\n",
    "    Y = torch.zeros(K, p)\n",
    "    for k in range(K):\n",
    "        X[k+1,:] = model.simulate_diffusion(X[k,:])\n",
    "        Y[k,:] = X[k+1,:] + multiplier * torch.sqrt(var_obs) * torch.randn(1,p)\n",
    "\n",
    "    for r in range(R):\n",
    "        # run particle filters\n",
    "        BPF_output = model.run_BPF(X0.repeat((N,1)), Y, N)\n",
    "        APF_output = model.run_APF(X0.repeat((N,1)), Y, N)\n",
    "        true_APF_output = run_true_APF(X0.repeat((N,1)), Y, N)\n",
    "        FA_APF_output = run_FA_APF(X0.repeat((N,1)), Y, N)\n",
    "\n",
    "        # save average ESS%\n",
    "        BPF_ESS = torch.mean(BPF_output['ess'] * 100 / N)\n",
    "        APF_ESS = torch.mean(APF_output['ess'] * 100 / N)\n",
    "        true_APF_ESS = torch.mean(true_APF_output['ess'] * 100 / N)\n",
    "        FA_APF_ESS = torch.mean(FA_APF_output['ess'] * 100 / N)\n",
    "        BPF['ess'][i,r] = BPF_ESS\n",
    "        APF['ess'][i,r] = APF_ESS\n",
    "        true_APF['ess'][i,r] = true_APF_ESS\n",
    "        FA_APF['ess'][i,r] = FA_APF_ESS\n",
    "\n",
    "        # save log-likelihood estimates\n",
    "        BPF_log_estimate = BPF_output['log_norm_const'][-1]\n",
    "        APF_log_estimate = APF_output['log_norm_const'][-1]\n",
    "        true_APF_log_estimate = true_APF_output['log_norm_const'][-1]\n",
    "        FA_APF_log_estimate = FA_APF_output['log_norm_const'][-1]\n",
    "        BPF['log_estimate'][i,r] = BPF_log_estimate\n",
    "        APF['log_estimate'][i,r] = APF_log_estimate\n",
    "        true_APF['log_estimate'][i,r] = true_APF_log_estimate\n",
    "        FA_APF['log_estimate'][i,r] = FA_APF_log_estimate\n",
    "\n",
    "        # print output\n",
    "        print('Multipler: ' + str(multiplier) + ' Repeat: ' + str(r)) \n",
    "        print('BPF ESS%: ' + str(BPF_ESS))\n",
    "        print('APF ESS%: ' + str(APF_ESS)) \n",
    "        print('True-APF ESS%: ' + str(true_APF_ESS)) \n",
    "        print('FA-APF ESS%: ' + str(APF_ESS)) \n",
    "        print('BPF log-estimate: ' + str(BPF_log_estimate))\n",
    "        print('APF log-estimate: ' + str(APF_log_estimate))\n",
    "        print('True-APF log-estimate: ' + str(true_APF_log_estimate))\n",
    "        print('FA-APF log-estimate: ' + str(FA_APF_log_estimate))\n",
    "\n",
    "# save results\n",
    "results = {'BPF' : BPF, 'APF' : APF, 'True-APF' : true_APF , 'FA-APF' : FA_APF}\n",
    "torch.save(results, 'OU_robust.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d3d6b27e62ee9b46d5d0cefa03313813aa6f690ce93c0eb61e5a28c453ab864"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
