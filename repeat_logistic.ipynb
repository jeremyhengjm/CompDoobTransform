{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import CompDoobTransform as cdt\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from CompDoobTransform.utils import negative_binomial_logpdf\n",
    "from torch.distributions.gamma import Gamma\n",
    "plt.style.use('ggplot')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Computing on ' + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning parameters\n",
    "T = torch.tensor(1.0, device = device) # time interval\n",
    "M = 50 # number of time steps\n",
    "filename = 'logistic.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for objects relating to latent state process\n",
    "state = {}\n",
    "\n",
    "# dimension of state \n",
    "d = 1 \n",
    "state['dim'] = d\n",
    "\n",
    "# model parameters \n",
    "theta = torch.tensor([2.397, 4.429e-03, 0.840, 17.631], device = device)\n",
    "\n",
    "# drift of diffusion (after Lamperti transformation)\n",
    "b_constant = theta[0] / theta[2] \n",
    "b_factor = theta[1] / theta[2]\n",
    "b = lambda x: b_constant - b_factor * torch.exp(theta[2] * x) # drift\n",
    "state['drift'] = b\n",
    "\n",
    "# diffusion coefficient of diffusion (after Lamperti transformation)\n",
    "sigma = torch.tensor(1.0, device = device) # diffusion coefficient\n",
    "state['sigma'] = sigma\n",
    "\n",
    "# stationary distribution (before Lamperti transformation)\n",
    "alpha = 2.0 * (0.5 * theta[2]**2 + theta[0]) / theta[2]**2 - 1.0\n",
    "beta = 2.0 * theta[1] / theta[2]**2\n",
    "stationary_distribution = Gamma(alpha, beta)\n",
    "\n",
    "# simulate initial states (from stationary distribution)\n",
    "initial = lambda N: torch.log(stationary_distribution.sample((N, 1))) / theta[2]\n",
    "state['initial'] = initial\n",
    "\n",
    "# time interval\n",
    "state['terminal_time'] = T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for objects relating to observations\n",
    "obs = {}\n",
    "\n",
    "# dimension of observation\n",
    "p = 1\n",
    "obs['dim'] = p\n",
    "\n",
    "# log-observation density\n",
    "obs_log_density = lambda x, y: negative_binomial_logpdf(y, theta[3], torch.exp(theta[2] * x)) \n",
    "obs['log_density'] = obs_log_density\n",
    "\n",
    "# simulate observations\n",
    "def simulate_obs(x, theta3):\n",
    "    size = theta3.numpy()\n",
    "    prob = (theta3 / (theta3 + torch.exp(theta[2] * x))).numpy()\n",
    "    out = torch.tensor(np.random.negative_binomial(n = size, p = prob))\n",
    "    return out\n",
    "\n",
    "observation = lambda N: simulate_obs(initial(N))\n",
    "obs['observation'] = observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning standardization\n",
    "N_large = 100000\n",
    "X = initial(N_large)\n",
    "Y = torch.tensor(observation(N_large), dtype = torch.float32)\n",
    "\n",
    "# means and standard deviations\n",
    "standardization = {'x_mean': torch.mean(X, 0), \n",
    "                   'x_std': torch.std(X, 0), \n",
    "                   'y_mean': torch.mean(Y, 0), \n",
    "                   'y_std': torch.std(Y, 0)}\n",
    "\n",
    "standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V0 and Z neural network configuration\n",
    "V0_net_config = {'layers': [16], 'standardization': standardization}\n",
    "Z_net_config = {'layers': [d+16], 'standardization': standardization}\n",
    "net_config = {'V0': V0_net_config, 'Z': Z_net_config}\n",
    "\n",
    "# learning type\n",
    "# learning_type = 'standard'\n",
    "learning_type = 'iterative'                \n",
    "\n",
    "# create model instance\n",
    "model = cdt.core.model(state, obs, M, net_config, device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization configuration (standard training)\n",
    "I = 2000\n",
    "optim_config = {'minibatch': 100, \n",
    "                'num_obs_per_batch': 10, \n",
    "                'num_iterations': I,\n",
    "                'learning_rate' : 0.01, \n",
    "                'initial_required' : True}\n",
    "# training\n",
    "time_start = time.time() \n",
    "if learning_type == 'standard':\n",
    "    model.train_standard(optim_config)\n",
    "if learning_type == 'iterative':\n",
    "    model.train_iterative(optim_config)\n",
    "time_end = time.time()\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Training time (secs): \" + str(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss over optimization iterations\n",
    "plt.figure()\n",
    "plt.plot(torch.arange(I), model.loss.to('cpu'), 'k.')\n",
    "plt.xlabel('iteration', fontsize = 15)\n",
    "plt.ylabel('loss', fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat particle filters\n",
    "num_obs = [100, 200, 400, 800, 1600]\n",
    "len_num_obs = len(num_obs)\n",
    "num_particles = [2**6, 2**7, 2**8, 2**9, 2**10]\n",
    "R = 100 # number of repeats\n",
    "BPF = {'ess' : torch.zeros(len_num_obs, R), 'log_estimate' : torch.zeros(len_num_obs, R)}\n",
    "APF = {'ess' : torch.zeros(len_num_obs, R), 'log_estimate' : torch.zeros(len_num_obs, R)}\n",
    "\n",
    "for i in range(len_num_obs):\n",
    "    # number of observations\n",
    "    K = num_obs[i]\n",
    "\n",
    "    # number of particles\n",
    "    N = num_particles[i]\n",
    "\n",
    "    # simulate latent process and observations\n",
    "    X0 = initial(1)\n",
    "    X = torch.zeros(K+1, d)\n",
    "    X[0,:] = X0.clone()\n",
    "    Y = torch.zeros(K, p)\n",
    "    for k in range(K):\n",
    "        X[k+1,:] = model.simulate_diffusion(X[k,:])\n",
    "        Y[k,:] = simulate_obs(X[k+1,:])\n",
    "\n",
    "    for r in range(R):\n",
    "        # run particle filters\n",
    "        BPF_output = model.run_BPF(X0.repeat((N,1)), Y, N)\n",
    "        APF_output = model.run_APF(X0.repeat((N,1)), Y, N)\n",
    "\n",
    "        # save average ESS%\n",
    "        BPF_ESS = torch.mean(BPF_output['ess'] * 100 / N)\n",
    "        APF_ESS = torch.mean(APF_output['ess'] * 100 / N)\n",
    "        BPF['ess'][i,r] = BPF_ESS\n",
    "        APF['ess'][i,r] = APF_ESS\n",
    "\n",
    "        # save log-likelihood estimates\n",
    "        BPF_log_estimate = BPF_output['log_norm_const'][-1]\n",
    "        APF_log_estimate = APF_output['log_norm_const'][-1]\n",
    "        BPF['log_estimate'][i,r] = BPF_log_estimate\n",
    "        APF['log_estimate'][i,r] = APF_log_estimate\n",
    "\n",
    "        # print output\n",
    "        print('No. of observations: ' + str(K) + ' Repeat: ' + str(r)) \n",
    "        print('BPF ESS%: ' + str(BPF_ESS))\n",
    "        print('APF ESS%: ' + str(APF_ESS)) \n",
    "        print('BPF log-estimate: ' + str(BPF_log_estimate))\n",
    "        print('APF log-estimate: ' + str(APF_log_estimate))\n",
    "\n",
    "# save results\n",
    "results = {'BPF' : BPF, 'APF' : APF}\n",
    "torch.save(results, filename)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88f8cf8e21ecdddc206b113d2af7f84f81c01e5536c8050713d03d2f94453c4b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('standard')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
