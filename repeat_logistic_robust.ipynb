{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import CompDoobTransform as cdt\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from CompDoobTransform.utils import negative_binomial_logpdf\n",
    "from torch.distributions.gamma import Gamma\n",
    "plt.style.use('ggplot')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Computing on ' + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for objects relating to latent state process\n",
    "state = {}\n",
    "\n",
    "# dimension of state \n",
    "d = 1 \n",
    "state['dim'] = d\n",
    "\n",
    "# model parameters \n",
    "theta = torch.tensor([2.397, 4.429e-03, 0.840, 17.631], device = device)\n",
    "\n",
    "# drift of diffusion (after Lamperti transformation)\n",
    "b_constant = theta[0] / theta[2] \n",
    "b_factor = theta[1] / theta[2]\n",
    "b = lambda x: b_constant - b_factor * torch.exp(theta[2] * x) # drift\n",
    "state['drift'] = b\n",
    "\n",
    "# diffusion coefficient of diffusion (after Lamperti transformation)\n",
    "sigma = torch.tensor(1.0, device = device) # diffusion coefficient\n",
    "state['sigma'] = sigma\n",
    "\n",
    "# stationary distribution (before Lamperti transformation)\n",
    "alpha = 2.0 * (0.5 * theta[2]**2 + theta[0]) / theta[2]**2 - 1.0\n",
    "beta = 2.0 * theta[1] / theta[2]**2\n",
    "stationary_distribution = Gamma(alpha, beta)\n",
    "\n",
    "# simulate initial states (from stationary distribution)\n",
    "initial = lambda N: torch.log(stationary_distribution.sample((N, 1))) / theta[2]\n",
    "state['initial'] = initial\n",
    "\n",
    "# time interval\n",
    "T = torch.tensor(1.0, device = device) \n",
    "state['terminal_time'] = T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for objects relating to observations\n",
    "obs = {}\n",
    "\n",
    "# dimension of observation\n",
    "p = 1\n",
    "obs['dim'] = p\n",
    "\n",
    "# log-observation density\n",
    "obs_log_density = lambda x, y: negative_binomial_logpdf(y, theta[3], torch.exp(theta[2] * x)) \n",
    "obs['log_density'] = obs_log_density\n",
    "\n",
    "# simulate observations\n",
    "def simulate_obs(x, theta3):\n",
    "    size = theta3.numpy()\n",
    "    prob = (theta3 / (theta3 + torch.exp(theta[2] * x))).numpy()\n",
    "    out = torch.tensor(np.random.negative_binomial(n = size, p = prob))\n",
    "    return out\n",
    "\n",
    "observation = lambda N: simulate_obs(initial(N), theta[3])\n",
    "obs['observation'] = observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning standardization\n",
    "N_large = 100000\n",
    "X = initial(N_large)\n",
    "Y = torch.tensor(observation(N_large), dtype = torch.float32)\n",
    "\n",
    "# means and standard deviations\n",
    "standardization = {'x_mean': torch.mean(X, 0), \n",
    "                   'x_std': torch.std(X, 0), \n",
    "                   'y_mean': torch.mean(Y, 0), \n",
    "                   'y_std': torch.std(Y, 0)}\n",
    "\n",
    "print(standardization)\n",
    "\n",
    "# observation mean and variance at stationarity\n",
    "obs_mean = torch.mean(torch.exp(theta[2] * X))\n",
    "obs_var = obs_mean + obs_mean**2  / theta[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-discretization settings\n",
    "M = 50 # number of time steps\n",
    "\n",
    "# V0 and Z neural network configuration\n",
    "V0_net_config = {'layers': [16], 'standardization': standardization}\n",
    "Z_net_config = {'layers': [d+16], 'standardization': standardization}\n",
    "net_config = {'V0': V0_net_config, 'Z': Z_net_config}\n",
    "\n",
    "# optimization configuration\n",
    "I = 2000\n",
    "optim_config = {'minibatch': 100, \n",
    "                'num_obs_per_batch': 10, \n",
    "                'num_iterations': I,\n",
    "                'learning_rate' : 0.01, \n",
    "                'initial_required' : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model instance\n",
    "model_static = cdt.core.model(state, obs, M, net_config, device = 'cpu')\n",
    "\n",
    "# static training\n",
    "time_start = time.time() \n",
    "model_static.train_standard(optim_config)\n",
    "time_end = time.time()\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Training time (secs): \" + str(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model instance\n",
    "model = cdt.core.model(state, obs, M, net_config, device = 'cpu')\n",
    "\n",
    "# iterative training\n",
    "time_start = time.time() \n",
    "model.train_iterative(optim_config)\n",
    "time_end = time.time()\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Training time (secs): \" + str(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss over optimization iterations\n",
    "plt.figure()\n",
    "plt.plot(torch.arange(I), model_static.loss.to('cpu'), '-')\n",
    "plt.plot(torch.arange(I), model.loss.to('cpu'), '-')\n",
    "plt.xlabel('iteration', fontsize = 15)\n",
    "plt.ylabel('loss', fontsize = 15)\n",
    "plt.legend(['Static CDT', 'Iterative CDT'], fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guided intermediate resampling filter\n",
    "inverse_temperature = torch.linspace(0.0, 1.0, M+1) # linear schedule\n",
    "\n",
    "def guiding_initial(x, y, p):\n",
    "    guiding = inverse_temperature[0]**p * obs_log_density(x,y)\n",
    "    return guiding\n",
    "\n",
    "def guiding_intermediate(m, x, x_next, y, p):\n",
    "    log_potential = inverse_temperature[m-1]**p * obs_log_density(x,y)\n",
    "    log_potential_next = inverse_temperature[m]**p * obs_log_density(x_next,y) \n",
    "    guiding = log_potential_next - log_potential\n",
    "    return guiding\n",
    "\n",
    "def guiding_obs_time(m, x, x_next, y, y_next, p):\n",
    "    guiding = guiding_intermediate(m, x, x_next, y, p) + guiding_initial(x_next, y_next, p)\n",
    "    return guiding\n",
    "\n",
    "guiding_linear = {}\n",
    "guiding_linear['initial'] = lambda x, y: guiding_initial(x, y, 1.0)\n",
    "guiding_linear['intermediate'] = lambda m, x, x_next, y: guiding_intermediate(m, x, x_next, y, 1.0)\n",
    "guiding_linear['obs_time'] = lambda m, x, x_next, y, y_next: guiding_obs_time(m, x, x_next, y, y_next, 1.0)\n",
    "\n",
    "guiding_quadratic = {}\n",
    "guiding_quadratic['initial'] = lambda x, y: guiding_initial(x, y, 2.0)\n",
    "guiding_quadratic['intermediate'] = lambda m, x, x_next, y: guiding_intermediate(m, x, x_next, y, 2.0)\n",
    "guiding_quadratic['obs_time'] = lambda m, x, x_next, y, y_next: guiding_obs_time(m, x, x_next, y, y_next, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat particle filters\n",
    "multiplier_level = list(range(1,7)) # controls level of misspecification\n",
    "num_multiplier = len(multiplier_level)\n",
    "K = 100 # number of observations\n",
    "R = 100 # number of repeats\n",
    "N = 2**6 # number of particles\n",
    "BPF = {'ess' : torch.zeros(num_multiplier, R), 'log_estimate' : torch.zeros(num_multiplier, R)}\n",
    "APF = {'ess' : torch.zeros(num_multiplier, R), 'log_estimate' : torch.zeros(num_multiplier, R)}\n",
    "APFF = {'ess' : torch.zeros(num_multiplier, R), 'log_estimate' : torch.zeros(num_multiplier, R)}\n",
    "GIRF1 = {'ess' : torch.zeros(num_multiplier, R), 'log_estimate' : torch.zeros(num_multiplier, R)}\n",
    "GIRF2 = {'ess' : torch.zeros(num_multiplier, R), 'log_estimate' : torch.zeros(num_multiplier, R)}\n",
    "\n",
    "for i in range(num_multiplier):\n",
    "    # level of misspecification\n",
    "    multiplier = float(multiplier_level[i])\n",
    "    implied_theta3 = obs_mean**2 / (multiplier**2 * obs_var - obs_mean)\n",
    "\n",
    "    # simulate latent process and observations\n",
    "    X0 = initial(1)\n",
    "    X = torch.zeros(K+1, d)\n",
    "    X[0,:] = X0.clone()\n",
    "    Y = torch.zeros(K, p)\n",
    "    for k in range(K):\n",
    "        X[k+1,:] = model.simulate_diffusion(X[k,:])\n",
    "        Y[k,:] = simulate_obs(X[k+1,:], implied_theta3)\n",
    "    \n",
    "    for r in range(R):\n",
    "        # run particle filters\n",
    "        BPF_output = model.run_BPF(X0.repeat((N,1)), Y, N)\n",
    "        APF_output = model.run_APF(X0.repeat((N,1)), Y, N)\n",
    "        APFF_output = model_static.run_APF(X0.repeat((N,1)), Y, N)\n",
    "        GIRF1_output = model.run_GIRF(X0.repeat((N,1)), Y, N, guiding_linear)\n",
    "        GIRF2_output = model.run_GIRF(X0.repeat((N,1)), Y, N, guiding_quadratic)\n",
    "\n",
    "        # save average ESS%\n",
    "        BPF_ESS = torch.mean(BPF_output['ess'] * 100 / N)\n",
    "        APF_ESS = torch.mean(APF_output['ess'] * 100 / N)\n",
    "        APFF_ESS = torch.mean(APFF_output['ess'] * 100 / N)\n",
    "        GIRF1_ESS = torch.mean(GIRF1_output['ess'] * 100 / N)\n",
    "        GIRF2_ESS = torch.mean(GIRF2_output['ess'] * 100 / N)\n",
    "        BPF['ess'][i,r] = BPF_ESS\n",
    "        APF['ess'][i,r] = APF_ESS\n",
    "        APFF['ess'][i,r] = APFF_ESS\n",
    "        GIRF1['ess'][i,r] = GIRF1_ESS\n",
    "        GIRF2['ess'][i,r] = GIRF2_ESS\n",
    "\n",
    "        # save log-likelihood estimates\n",
    "        BPF_log_estimate = BPF_output['log_norm_const'][-1]\n",
    "        APF_log_estimate = APF_output['log_norm_const'][-1]\n",
    "        APFF_log_estimate = APFF_output['log_norm_const'][-1]\n",
    "        GIRF1_log_estimate = GIRF1_output['log_norm_const'][-1]\n",
    "        GIRF2_log_estimate = GIRF2_output['log_norm_const'][-1]\n",
    "        BPF['log_estimate'][i,r] = BPF_log_estimate\n",
    "        APF['log_estimate'][i,r] = APF_log_estimate\n",
    "        APFF['log_estimate'][i,r] = APFF_log_estimate\n",
    "        GIRF1['log_estimate'][i,r] = GIRF1_log_estimate\n",
    "        GIRF2['log_estimate'][i,r] = GIRF2_log_estimate\n",
    "\n",
    "        # print output\n",
    "        print('Multipler: ' + str(multiplier) + ' Repeat: ' + str(r)) \n",
    "        print('Implied theta3: ' + str(implied_theta3))\n",
    "        print('BPF ESS%: ' + str(BPF_ESS))\n",
    "        print('APF ESS%: ' + str(APF_ESS)) \n",
    "        print('APFF ESS%: ' + str(APFF_ESS)) \n",
    "        print('GIRF1 ESS%: ' + str(GIRF1_ESS)) \n",
    "        print('GIRF2 ESS%: ' + str(GIRF2_ESS)) \n",
    "        print('BPF log-estimate: ' + str(BPF_log_estimate))\n",
    "        print('APF log-estimate: ' + str(APF_log_estimate))\n",
    "        print('APFF log-estimate: ' + str(APFF_log_estimate))\n",
    "        print('GIRF1 log-estimate: ' + str(GIRF1_log_estimate))\n",
    "        print('GIRF2 log-estimate: ' + str(GIRF2_log_estimate))\n",
    "\n",
    "# save results\n",
    "results = {'BPF' : BPF, 'APF' : APF, 'APFF' : APFF, 'GIRF1' : GIRF1, 'GIRF2' : GIRF2}\n",
    "torch.save(results, 'logistic_robust.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88f8cf8e21ecdddc206b113d2af7f84f81c01e5536c8050713d03d2f94453c4b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('standard')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
