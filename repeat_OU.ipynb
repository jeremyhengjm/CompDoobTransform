{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import CompDoobTransform as cdt\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from CompDoobTransform.utils import normal_logpdf, resampling\n",
    "plt.style.use('ggplot')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Computing on ' + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning parameters \n",
    "d = 2 \n",
    "# std_obs = 0.125\n",
    "std_obs = 1.0\n",
    "filename = 'OU_var_obs_small.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for objects relating to latent state process\n",
    "state = {}\n",
    "\n",
    "# dimension of state \n",
    "state['dim'] = d\n",
    "\n",
    "# drift of diffusion\n",
    "beta = torch.tensor(1.0, device = device) \n",
    "b = lambda x: - beta * x # drift\n",
    "state['drift'] = b\n",
    "\n",
    "# diffusion coefficient of diffusion\n",
    "sigma = torch.tensor(1.0, device = device) # diffusion coefficient\n",
    "state['sigma'] = sigma\n",
    "\n",
    "# simulate initial states (from stationary distribution)\n",
    "initial = lambda N: (sigma / torch.sqrt(2.0 * beta)) * torch.randn(N, d, device = device)\n",
    "state['initial'] = initial\n",
    "\n",
    "# time interval\n",
    "T = torch.tensor(1.0, device = device) \n",
    "state['terminal_time'] = T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for objects relating to observations\n",
    "obs = {}\n",
    "\n",
    "# dimension of observation\n",
    "p = d\n",
    "obs['dim'] = p\n",
    "\n",
    "# observation parameters\n",
    "var_obs = torch.tensor(std_obs**2, device = device) # variance of observation\n",
    "\n",
    "# log-observation density\n",
    "obs_log_density = lambda x, y: normal_logpdf(y, x, var_obs) # terminal condition, returns size (N)\n",
    "obs['log_density'] = obs_log_density\n",
    "\n",
    "# simulate observations\n",
    "observation = lambda N: initial(N) + torch.sqrt(var_obs) * torch.randn(N, p, device = device)\n",
    "obs['observation'] = observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define standardization means and standard deviations\n",
    "standardization = {'x_mean': torch.zeros(1), \n",
    "                   'x_std': (sigma / torch.sqrt(2.0 * beta)) * torch.ones(1), \n",
    "                   'y_mean': torch.zeros(1), \n",
    "                   'y_std': torch.sqrt(sigma**2 / (2.0 * beta) + var_obs) * torch.ones(1)}\n",
    "\n",
    "standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition mean and variance \n",
    "mean_x = lambda t,x: x * torch.exp(- beta * t)\n",
    "var_x = lambda t: (1.0 - torch.exp(- 2.0 * beta * t)) / (2 * beta)\n",
    "\n",
    "# posterior mean and variance at time T\n",
    "var_h = lambda t: 1.0 / (1.0 / var_x(t) + 1.0 / var_obs)\n",
    "mean_h = lambda x0,y: var_h(T) * (mean_x(T, x0) / var_x(T) + y / var_obs)\n",
    "\n",
    "# h function\n",
    "def log_h(t,x,y): \n",
    "    if t < T:\n",
    "        output = - 0.5 * d * torch.log(torch.tensor(2 * math.pi, device = device)) \\\n",
    "                 - 0.5 * d * torch.log(var_x(T-t)) - 0.5 * d * torch.log(var_obs) + 0.5 * d * torch.log(var_h(T-t)) \\\n",
    "                 + 0.5 * var_h(T-t) * torch.sum(torch.square(mean_x(T-t,x) / var_x(T-t) + y / var_obs), dim = 1, keepdim = True) \\\n",
    "                 - 0.5 * torch.sum(torch.square(mean_x(T-t,x)), dim = 1, keepdim = True) / var_x(T-t) \\\n",
    "                 - 0.5 * torch.sum(torch.square(y)) / var_obs\n",
    "    else: \n",
    "        output = obs_log_density(x,y)\n",
    "    return output\n",
    "\n",
    "# true V0 network\n",
    "V0_net_true = lambda x,y: -log_h(0.0, x, y.reshape(1,p)).squeeze()\n",
    "\n",
    "# true Z network \n",
    "Z_net_true = lambda t,x,y: - var_h(T-t) * (mean_x(T-t,x) / var_x(T-t) + y / var_obs) * torch.exp(- beta * (T-t)) / var_x(T-t) \\\n",
    "                           + mean_x(T-t,x) * torch.exp(- beta * (T-t)) / var_x(T-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-discretization settings\n",
    "M = 50 # number of time steps\n",
    "\n",
    "# V0 and Z neural network configuration\n",
    "V0_net_config = {'layers': [16], 'standardization': standardization}\n",
    "Z_net_config = {'layers': [d+16], 'standardization': standardization}\n",
    "net_config = {'V0': V0_net_config, 'Z': Z_net_config}\n",
    "\n",
    "# learning type\n",
    "# learning_type = 'standard'\n",
    "learning_type = 'iterative'                \n",
    "\n",
    "# create model instance\n",
    "model = cdt.core.model(state, obs, M, net_config, device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization configuration (standard training)\n",
    "I = 2000\n",
    "optim_config = {'minibatch': 100, \n",
    "                'num_obs_per_batch': 10, \n",
    "                'num_iterations': I,\n",
    "                'learning_rate' : 0.01, \n",
    "                'initial_required' : True}\n",
    "# training\n",
    "time_start = time.time() \n",
    "if learning_type == 'standard':\n",
    "    model.train_standard(optim_config)\n",
    "if learning_type == 'iterative':\n",
    "    model.train_iterative(optim_config)\n",
    "time_end = time.time()\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Training time (secs): \" + str(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss over optimization iterations\n",
    "plt.figure()\n",
    "plt.plot(torch.arange(I), model.loss.to('cpu'), 'k.')\n",
    "plt.xlabel('iteration', fontsize = 15)\n",
    "plt.ylabel('loss', fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary particle filter with true networks\n",
    "def run_true_APF(initial_states, observations, num_samples):\n",
    "    \"\"\"\n",
    "    Run auxiliary particle filter with true networks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    initial_states : initial states of X process (N, d)\n",
    "    \n",
    "    observations : sequence of observations to be filtered (K, p)\n",
    "\n",
    "    num_samples : sample size (int)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict containing:    \n",
    "        states : X process at observation times (N, K+1, d) or (N, K*M+1, d) if full_path == True\n",
    "        ess : effective sample sizes at unit times (K+1)        \n",
    "        log_norm_const : log-normalizing constant estimates (K+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize and preallocate\n",
    "    N = num_samples\n",
    "    Y = observations\n",
    "    K = observations.shape[0]        \n",
    "    X = initial_states        \n",
    "    states = torch.zeros(N, K+1, d, device = device)\n",
    "    states[:, 0, :] = X\n",
    "    ess = torch.zeros(K+1, device = device)\n",
    "    ess[0] = N\n",
    "    log_norm_const = torch.zeros(K+1, device = device)\n",
    "    log_ratio_norm_const = torch.tensor(0.0, device = device)\n",
    "    \n",
    "    # each observation\n",
    "    for k in range(K):        \n",
    "\n",
    "        # evaluate initial values V0\n",
    "        V0 = V0_net_true(X, Y[k,:]) # size (N)            \n",
    "        V = V0.clone()\n",
    "        \n",
    "        # each time interval\n",
    "        for m in range(M):\n",
    "            # time step \n",
    "            stepsize = model.stepsizes[m]\n",
    "            t = model.time[m]\n",
    "\n",
    "            # Brownian increment\n",
    "            W = torch.sqrt(stepsize) * torch.randn(N, d, device = device) # size (N, d)\n",
    "            \n",
    "            # simulate V process forwards in time            \n",
    "            Z = Z_net_true(t, X, Y[k,:]) # size (N, d)\n",
    "            control = - Z.clone()\n",
    "            drift_V = - 0.5 * torch.sum(torch.square(Z), 1) # size (N)                \n",
    "            euler_V = V + stepsize * drift_V # size (N)\n",
    "            V = euler_V + torch.sum(Z * W, 1) # size (N)\n",
    "\n",
    "            # simulate X process forwards in time\n",
    "            drift_X = b(X) + sigma * control\n",
    "            euler_X = X + stepsize * drift_X\n",
    "            X = euler_X + sigma * W\n",
    "\n",
    "        # compute and normalize weights, compute ESS and normalizing constant\n",
    "        log_weights = V + obs_log_density(X, Y[k,:]) - V0\n",
    "        max_log_weights = torch.max(log_weights)\n",
    "        weights = torch.exp(log_weights - max_log_weights)\n",
    "        normalized_weights = weights / torch.sum(weights)\n",
    "        ess[k+1] = 1.0 / torch.sum(normalized_weights**2)\n",
    "        log_ratio_norm_const = log_ratio_norm_const + torch.log(torch.mean(weights)) + max_log_weights\n",
    "        log_norm_const[k+1] = log_ratio_norm_const\n",
    "\n",
    "        # resampling            \n",
    "        ancestors = resampling(normalized_weights, N)\n",
    "        X = X[ancestors,:]\n",
    "\n",
    "        # store states \n",
    "        states[:, k+1, :] = X\n",
    "\n",
    "    # output\n",
    "    output = {'states' : states, 'ess' : ess, 'log_norm_const' : log_norm_const}\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully adapted auxiliary particle filter without time-discretization\n",
    "def run_FA_APF(initial_states, observations, num_samples):\n",
    "    \"\"\"\n",
    "    Run fully adapted auxiliary particle filter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    initial_states : initial states of X process (N, d)\n",
    "    \n",
    "    observations : sequence of observations to be filtered (K, p)\n",
    "\n",
    "    num_samples : sample size (int)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict containing:    \n",
    "        states : X process at observation times (N, K+1, d) or (N, K*M+1, d) if full_path == True\n",
    "        ess : effective sample sizes at unit times (K+1)        \n",
    "        log_norm_const : log-normalizing constant estimates (K+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize and preallocate\n",
    "    N = num_samples\n",
    "    Y = observations\n",
    "    K = observations.shape[0]        \n",
    "    X = initial_states        \n",
    "    states = torch.zeros(N, K+1, d, device = device)\n",
    "    states[:, 0, :] = X\n",
    "    ess = torch.zeros(K+1, device = device)\n",
    "    ess[0] = N\n",
    "    log_norm_const = torch.zeros(K+1, device = device)\n",
    "    log_ratio_norm_const = torch.tensor(0.0, device = device)\n",
    "    \n",
    "    # each observation\n",
    "    for k in range(K): \n",
    "        # compute and normalize weights, compute ESS and normalizing constant\n",
    "        log_weights = log_h(0.0, X, Y[k,:].reshape(1,p)).squeeze()\n",
    "        max_log_weights = torch.max(log_weights)\n",
    "        weights = torch.exp(log_weights - max_log_weights)\n",
    "        normalized_weights = weights / torch.sum(weights)\n",
    "        ess[k+1] = 1.0 / torch.sum(normalized_weights**2)\n",
    "        log_ratio_norm_const = log_ratio_norm_const + torch.log(torch.mean(weights)) + max_log_weights\n",
    "        log_norm_const[k+1] = log_ratio_norm_const\n",
    "\n",
    "        # resampling            \n",
    "        ancestors = resampling(normalized_weights, N)\n",
    "        X = X[ancestors,:]\n",
    "\n",
    "        # move using locally optimal proposal transition\n",
    "        X = mean_h(X, Y[k,:].reshape(1,p)) + torch.sqrt(var_h(T)) * torch.randn(N, d, device = device)\n",
    "\n",
    "        # store states \n",
    "        states[:, k+1, :] = X\n",
    "\n",
    "    # output\n",
    "    output = {'states' : states, 'ess' : ess, 'log_norm_const' : log_norm_const}\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimally guided intermediate resampling filter\n",
    "def guiding_initial(x,y):\n",
    "    y = y.reshape(1,p)\n",
    "    guiding = log_h(0.0, x, y).squeeze()\n",
    "    return guiding\n",
    "\n",
    "def guiding_intermediate(m, x, x_next, y):\n",
    "    t = float(model.time[m-1])\n",
    "    t_next = float(model.time[m])\n",
    "    y = y.reshape(1,p)\n",
    "    log_potential = log_h(t, x, y).squeeze()\n",
    "    log_potential_next = log_h(t_next, x_next, y).squeeze()\n",
    "    guiding = log_potential_next - log_potential\n",
    "    return guiding\n",
    "\n",
    "def guiding_obs_time(m, x, x_next, y, y_next):\n",
    "    guiding = guiding_intermediate(m, x, x_next, y) + guiding_initial(x_next,y_next)\n",
    "    return guiding\n",
    "\n",
    "guiding_function = {'initial': guiding_initial, \n",
    "                    'intermediate': guiding_intermediate,\n",
    "                    'obs_time': guiding_obs_time}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat particle filters\n",
    "multiplier = 1.0\n",
    "num_obs = [100, 200, 400, 800, 1600]\n",
    "len_num_obs = len(num_obs)\n",
    "num_particles = [2**6, 2**7, 2**8, 2**9, 2**10]\n",
    "R = 100 # number of repeats\n",
    "BPF = {'ess' : torch.zeros(len_num_obs, R), 'log_estimate' : torch.zeros(len_num_obs, R)}\n",
    "APF = {'ess' : torch.zeros(len_num_obs, R), 'log_estimate' : torch.zeros(len_num_obs, R)}\n",
    "true_APF = {'ess' : torch.zeros(len_num_obs, R), 'log_estimate' : torch.zeros(len_num_obs, R)}\n",
    "FA_APF = {'ess' : torch.zeros(len_num_obs, R), 'log_estimate' : torch.zeros(len_num_obs, R)}\n",
    "GIRF = {'ess' : torch.zeros(len_num_obs, R), 'log_estimate' : torch.zeros(len_num_obs, R)}\n",
    "\n",
    "for i in range(len_num_obs):\n",
    "    # number of observations\n",
    "    K = num_obs[i]\n",
    "\n",
    "    # number of particles\n",
    "    N = num_particles[i]\n",
    "\n",
    "    # simulate latent process and observations\n",
    "    X0 = initial(1)\n",
    "    X = torch.zeros(K+1, d)\n",
    "    X[0,:] = X0.clone()\n",
    "    Y = torch.zeros(K, p)\n",
    "    for k in range(K):\n",
    "        X[k+1,:] = model.simulate_diffusion(X[k,:])\n",
    "        Y[k,:] = X[k+1,:] + multiplier * torch.sqrt(var_obs) * torch.randn(1,p)\n",
    "\n",
    "    for r in range(R):\n",
    "        # run particle filters\n",
    "        BPF_output = model.run_BPF(X0.repeat((N,1)), Y, N)\n",
    "        APF_output = model.run_APF(X0.repeat((N,1)), Y, N)\n",
    "        true_APF_output = run_true_APF(X0.repeat((N,1)), Y, N)\n",
    "        FA_APF_output = run_FA_APF(X0.repeat((N,1)), Y, N)\n",
    "        GIRF_output = model.run_GIRF(X0.repeat((N,1)), Y, N, guiding_function)\n",
    "\n",
    "        # save average ESS%\n",
    "        BPF_ESS = torch.mean(BPF_output['ess'] * 100 / N)\n",
    "        APF_ESS = torch.mean(APF_output['ess'] * 100 / N)\n",
    "        true_APF_ESS = torch.mean(true_APF_output['ess'] * 100 / N)\n",
    "        FA_APF_ESS = torch.mean(FA_APF_output['ess'] * 100 / N)\n",
    "        GIRF_ESS = torch.mean(GIRF_output['ess'] * 100 / N)\n",
    "        BPF['ess'][i,r] = BPF_ESS\n",
    "        APF['ess'][i,r] = APF_ESS\n",
    "        true_APF['ess'][i,r] = true_APF_ESS\n",
    "        FA_APF['ess'][i,r] = FA_APF_ESS\n",
    "        GIRF['ess'][i,r] = GIRF_ESS\n",
    "\n",
    "        # save log-likelihood estimates\n",
    "        BPF_log_estimate = BPF_output['log_norm_const'][-1]\n",
    "        APF_log_estimate = APF_output['log_norm_const'][-1]\n",
    "        true_APF_log_estimate = true_APF_output['log_norm_const'][-1]\n",
    "        FA_APF_log_estimate = FA_APF_output['log_norm_const'][-1]\n",
    "        GIRF_log_estimate = GIRF_output['log_norm_const'][-1]\n",
    "        BPF['log_estimate'][i,r] = BPF_log_estimate\n",
    "        APF['log_estimate'][i,r] = APF_log_estimate\n",
    "        true_APF['log_estimate'][i,r] = true_APF_log_estimate\n",
    "        FA_APF['log_estimate'][i,r] = FA_APF_log_estimate\n",
    "        GIRF['log_estimate'][i,r] = GIRF_log_estimate\n",
    "\n",
    "        # print output\n",
    "        print('No. of observations: ' + str(K) + ' Repeat: ' + str(r)) \n",
    "        print('BPF ESS%: ' + str(BPF_ESS))\n",
    "        print('APF ESS%: ' + str(APF_ESS)) \n",
    "        print('True-APF ESS%: ' + str(true_APF_ESS)) \n",
    "        print('FA-APF ESS%: ' + str(FA_APF_ESS)) \n",
    "        print('GIRF ESS%: ' + str(GIRF_ESS)) \n",
    "        print('BPF log-estimate: ' + str(BPF_log_estimate))\n",
    "        print('APF log-estimate: ' + str(APF_log_estimate))\n",
    "        print('True-APF log-estimate: ' + str(true_APF_log_estimate))\n",
    "        print('FA-APF log-estimate: ' + str(FA_APF_log_estimate))\n",
    "        print('GIRF log-estimate: ' + str(GIRF_log_estimate))\n",
    "\n",
    "# save results\n",
    "results = {'BPF' : BPF, 'APF' : APF, 'True-APF' : true_APF , 'FA-APF' : FA_APF, 'GIRF' : GIRF}\n",
    "torch.save(results, filename)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88f8cf8e21ecdddc206b113d2af7f84f81c01e5536c8050713d03d2f94453c4b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('standard')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
